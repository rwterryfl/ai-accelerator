apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/accelerator-name: ""
    opendatahub.io/apiProtocol: REST
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
    opendatahub.io/template-display-name: Qwen2.5-0.5B-Instruct for TrustyAI LMEval lab
    opendatahub.io/template-name: qwen-runtime
    openshift.io/display-name: Qwen2.5-0.5B-Instruct for TrustyAI LMEval lab
  labels:
    opendatahub.io/dashboard: "true"
  name: qwen-instruct
spec:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "8080"
    openshift.io/display-name: Qwen2.5-0.5B-Instruct for TrustyAI LMEval lab
  labels:
    opendatahub.io/dashboard: "true"
  containers:
  - name: kserve-container
    image: quay.io/modh/vllm@sha256:4f1f6b5738b311332b2bc786ea71259872e570081807592d97b4bd4cb65c4be1
    command:
      - python
      - "-m"
      - vllm.entrypoints.openai.api_server
    args:
      - "--port=8080"
      - "--model=/mnt/models"
      - "--served-model-name={{.Name}}"
    env:
      - name: HF_HOME
        value: /tmp/hf_home
    ports:
      - containerPort: 8080
        protocol: TCP
    volumeMounts:
      - mountPath: /dev/shm
        name: shm
  multiModel: false
  supportedModelFormats:
    - autoSelect: true
      name: vLLM
  volumes:
    - emptyDir:
        medium: Memory
        sizeLimit: 2Gi
      name: shm
